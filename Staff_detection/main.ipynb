{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba15a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"numpy<2.0\" --upgrade\n",
    "# !pip install torchreid\n",
    "# !pip install torch==2.0.1 torchvision==0.15.2\n",
    "# !pip install gdown\n",
    "# !pip uninstall torchreid -y\n",
    "# !pip install git+https://github.com/KaiyangZhou/deep-person-reid.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be4f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"OFJsbzSXtei8j554tCdF\")\n",
    "project = rf.workspace().project(\"walking-staff-detection-ms3uf\")\n",
    "model = project.version(2).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba611126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchreid.utils import FeatureExtractor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "extractor = FeatureExtractor(\n",
    "    model_name='osnet_x1_0',\n",
    "    pretrained=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "staff_db = []\n",
    "\n",
    "def get_embedding(image):\n",
    "    \"\"\"\n",
    "    image: numpy array (H, W, C), BGR 或 RGB 都可以\n",
    "    \"\"\"\n",
    "    return extractor(image)[0].cpu().numpy()\n",
    "\n",
    "def is_staff(feature, threshold=0.7):\n",
    "    if not staff_db:\n",
    "        return False\n",
    "    sims = cosine_similarity([feature], staff_db)[0]\n",
    "    return np.max(sims) > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eff1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_overlap(box1, box2):\n",
    "    \"\"\"\n",
    "    box = [x1, y1, x2, y2]\n",
    "    return: True if IoU > 0\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    if x2 < x1 or y2 < y1:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbcae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = \"test.mp4\"  # 上傳影片到 Colab\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Roboflow 預測\n",
    "    results = model.predict(frame, confidence=40, overlap=30).json()\n",
    "\n",
    "    people = [obj for obj in results['predictions'] if obj['class'] == 'person']\n",
    "    tags = [obj for obj in results['predictions'] if obj['class'] == 'tag']\n",
    "\n",
    "    for p in people:\n",
    "        x, y, w, h = int(p['x']), int(p['y']), int(p['width']), int(p['height'])\n",
    "        person_crop = frame[y-h//2:y+h//2, x-w//2:x+w//2]\n",
    "\n",
    "        feature = get_embedding(person_crop)\n",
    "\n",
    "        if not is_staff(feature):\n",
    "            for t in tags:\n",
    "                tx, ty, tw, th = int(t['x']), int(t['y']), int(t['width']), int(t['height'])\n",
    "                tag_box = [tx-tw//2, ty-th//2, tx+tw//2, ty+th//2]\n",
    "                person_box = [x-w//2, y-h//2, x+w//2, y+h//2]\n",
    "\n",
    "                if boxes_overlap(person_box, tag_box):\n",
    "                    staff_db.append(feature)\n",
    "                    break\n",
    "\n",
    "cap.release()\n",
    "print(f\"Staff DB size: {len(staff_db)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c47d816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for video inference results for job 9af1d193-c809-4f43-9eab-a3242eaf1bad every 60s\n",
      "(0s): Checking for inference results\n"
     ]
    }
   ],
   "source": [
    "job_id, signed_url, expire_time = model.predict_video(\n",
    "    \"test.mp4\",\n",
    "    fps=25,\n",
    "    prediction_type=\"batch-video\",\n",
    ")\n",
    "\n",
    "results = model.poll_until_video_results(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0a3fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data.json', 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ad5791c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_id  :  7baac446-8217-41e6-8857-f72c6339be12\n",
      "time  :  0.2827616059948923\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 679.0, 'y': 94.0, 'width': 104.0, 'height': 136.0, 'confidence': 0.8160362839698792, 'class': 'people', 'class_id': 0, 'detection_id': 'aad9c306-8677-4f80-a349-2a489202e0fc'}]\n",
      "inference_id  :  7baac446-8217-41e6-8857-f72c6339be12\n",
      "time  :  0.28276918600022327\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 678.0, 'y': 96.0, 'width': 100.0, 'height': 134.0, 'confidence': 0.8084633946418762, 'class': 'people', 'class_id': 0, 'detection_id': 'c7122bf5-e451-404e-9ef1-e24759de9ac3'}]\n",
      "inference_id  :  7baac446-8217-41e6-8857-f72c6339be12\n",
      "time  :  0.28277114700176753\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 677.0, 'y': 97.0, 'width': 102.0, 'height': 132.0, 'confidence': 0.812053918838501, 'class': 'people', 'class_id': 0, 'detection_id': 'e2e5909f-db56-4d84-b297-e00c9133bff6'}]\n",
      "inference_id  :  7baac446-8217-41e6-8857-f72c6339be12\n",
      "time  :  0.2827728410047712\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 678.0, 'y': 97.5, 'width': 104.0, 'height': 131.0, 'confidence': 0.8254296779632568, 'class': 'people', 'class_id': 0, 'detection_id': '2ad72bc4-03d8-40f3-a4f7-5f9dd5167104'}]\n",
      "inference_id  :  adec846c-6a77-4f7f-af4c-34aabbe66784\n",
      "time  :  0.2752852480043657\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 676.0, 'y': 98.5, 'width': 104.0, 'height': 129.0, 'confidence': 0.8257006406784058, 'class': 'people', 'class_id': 0, 'detection_id': '547fa112-402a-45f2-839f-e88bcfa99cf1'}, {'x': 690.0, 'y': 89.0, 'width': 14.0, 'height': 16.0, 'confidence': 0.4590245485305786, 'class': 'tag', 'class_id': 1, 'detection_id': '66fc2c83-a268-464f-8c68-29b91d8f229c'}]\n",
      "inference_id  :  adec846c-6a77-4f7f-af4c-34aabbe66784\n",
      "time  :  0.27529418299673125\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 668.5, 'y': 97.5, 'width': 115.0, 'height': 131.0, 'confidence': 0.8238118886947632, 'class': 'people', 'class_id': 0, 'detection_id': 'c8a4b707-c112-435a-af12-cf9362b53036'}]\n",
      "inference_id  :  adec846c-6a77-4f7f-af4c-34aabbe66784\n",
      "time  :  0.27529603299626615\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 655.0, 'y': 101.5, 'width': 138.0, 'height': 137.0, 'confidence': 0.839332103729248, 'class': 'people', 'class_id': 0, 'detection_id': '3208a4c5-0ac1-4ab7-b625-b60314374e52'}, {'x': 680.0, 'y': 94.0, 'width': 14.0, 'height': 16.0, 'confidence': 0.5109317898750305, 'class': 'tag', 'class_id': 1, 'detection_id': '232f962b-dcad-49a6-8142-3c93b318a7d8'}]\n",
      "inference_id  :  adec846c-6a77-4f7f-af4c-34aabbe66784\n",
      "time  :  0.2752979289944051\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 646.5, 'y': 103.0, 'width': 135.0, 'height': 138.0, 'confidence': 0.825941801071167, 'class': 'people', 'class_id': 0, 'detection_id': 'a7539d03-7588-44be-ae5c-6b3d76990657'}, {'x': 667.0, 'y': 95.0, 'width': 16.0, 'height': 16.0, 'confidence': 0.6944519877433777, 'class': 'tag', 'class_id': 1, 'detection_id': '4b2f187e-8378-420c-9fa2-a5f50345bfe5'}]\n",
      "inference_id  :  172d9e60-4f8c-4059-8493-d93e2a477e35\n",
      "time  :  0.2895101930043893\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 645.5, 'y': 103.5, 'width': 131.0, 'height': 141.0, 'confidence': 0.7982075214385986, 'class': 'people', 'class_id': 0, 'detection_id': 'ffbd289e-4e94-4127-8ac0-9c222a70ba20'}, {'x': 659.0, 'y': 93.5, 'width': 14.0, 'height': 15.0, 'confidence': 0.48005056381225586, 'class': 'tag', 'class_id': 1, 'detection_id': '3510b7ae-1139-4c35-b18a-512419b597b4'}]\n",
      "inference_id  :  172d9e60-4f8c-4059-8493-d93e2a477e35\n",
      "time  :  0.289517714001704\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 635.0, 'y': 103.5, 'width': 118.0, 'height': 145.0, 'confidence': 0.8549673557281494, 'class': 'people', 'class_id': 0, 'detection_id': '27588ee4-9948-4aec-b1d9-307f74c1bd65'}, {'x': 650.5, 'y': 94.0, 'width': 17.0, 'height': 18.0, 'confidence': 0.6662863492965698, 'class': 'tag', 'class_id': 1, 'detection_id': '38f3039e-a701-42cb-8537-8984c71d81b9'}]\n",
      "inference_id  :  172d9e60-4f8c-4059-8493-d93e2a477e35\n",
      "time  :  0.2895196280005621\n",
      "image  :  {'width': 960, 'height': 720}\n",
      "predictions  :  [{'x': 623.0, 'y': 112.0, 'width': 106.0, 'height': 160.0, 'confidence': 0.8034837245941162, 'class': 'people', 'class_id': 0, 'detection_id': '81386108-2f16-45f2-a1de-6a9a792dbdb7'}, {'x': 637.5, 'y': 96.5, 'width': 17.0, 'height': 17.0, 'confidence': 0.6450353860855103, 'class': 'tag', 'class_id': 1, 'detection_id': 'd885b5d7-af8b-4578-9c19-5b1c06b2cc16'}]\n"
     ]
    }
   ],
   "source": [
    "project = \"walking-staff-detection-ms3uf\"\n",
    "\n",
    "for frame in range(len(results[\"frame_offset\"])):\n",
    "    for obj in results[project][frame]:\n",
    "        print(obj, \" : \", results[project][frame][obj])\n",
    "    if frame == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4715d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nthu_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
